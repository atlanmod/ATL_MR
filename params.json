{"name":"ATL-MR","tagline":"ATL on MapReduce ","body":"#ATL/MapReduce\r\n\r\nATL/MapReduce (ATL/MR) is a prototype tool for running complex **ATL** transformation in the cloud using **Hadoop** MapReduce.\r\nATL/MapReduce is implemented on top  of an extended ATL VM that can be found [here](https://github.com/atlanmod/org.eclipse.atl.atlMR/tree/master).\r\nCoupling **ATL/MR** with the [the extended VM](https://github.com/atlanmod/org.eclipse.atl.atlMR/tree/master) has proved a good performance, especially in terms of execution time. [In our experiments](http://www.emn.fr/z-info/atlanmod/index.php/Image:Atlmr-experiments-raw-data.zip), **ATL/MR** runs up to **~6x** faster compared to the regular VM while distributing it over 8 machines.  \r\n\r\n##How to use\r\n\r\nThe transformation configuration in ATL/MapReduce has one additional input w.r.t. the standalone one, a record file. \r\nThis file has the job of  defining the subset of model elements to be processed by a map worker.\r\n\r\n###Record file \r\n\r\nThe record file contains input model elements URIs as plain string, one per line. This file will be split by hadoop in input splits. Each map worker is assigned a chunk. Usage below:\r\n\r\n``java -jar atl-mr.jar -s <source.ecore> -i <input.xmi> [-o <records.rec>]``\r\n\r\n  Argument                            |  Meaning\r\n -------------------------------------|:-----------------------------------\r\n -s,--source-metamodel <source.ecore> |  URI of the source metamodel file.\r\n -i,--input <input.xmi>               |  URI of the input file.\r\n -o,--output <records.rec>            |  Path of the output records file.\r\n\r\n###Model transformation\r\n\r\nThe transformation parameters are provided by the means of arguments. Below the usage:\r\n\r\n``yarn jar atl-mr.jar -f <transformation.emftvm> -s <source.ecore> -t <target.ecore> -r <records.rec> -i <input.xmi> [-o <output.xmi>] [-m <mappers_hint> | -n <recors_per_mapper>]  [-v | -q]``\r\n \r\n  Argument                                    |  Meaning\r\n ---------------------------------------------|:-----------------------------------\r\n -f,--file <transformation.emftvm>            | URI of the ATL transformation file.       \r\n -s,--source-metamodel <source.ecore>         | URI of the source metamodel file.\r\n -t,--target-metamodel <target.ecore>         | URI of the target metamodel file.\r\n -r,--records <records.rec>                   | URI of the records file.\r\n -i,--input <input.xmi>                       | URI of the input file.\r\n -o,--output <output.xmi>                     | URI of the output file. Optional.\r\n -m,--recommended-mappers <mappers_hint>      | The recommended number of mappers (not strict, used only as a hint). Optional, defaults to 1. Excludes the use of '-n'.\r\n -n,--records-per-mapper <recors_per_mapper>  | Number of records to be processed by mapper. Optional, defaults to all records. Excludes the use of '-m'.\r\n -v,--verbose                                 | Verbose mode. Optional, disabled by default.\r\n -q,--quiet                                   |  Do not print any information about the transformation execution on the standard output. Optional, disabled by default.\r\n\r\n**Please note that resource URIs with the 'hdfs://' protocol are supported**. \r\n##Execution modes\r\n\r\nYou can run ATL/MapReduce in two different modes, within eclipse or in a hadoop cluster.\r\n\r\n###Within eclipse \r\nATL/MapReduce can be executed within eclipse. Hadoop configuration files are already provided for Win-x86-64. \r\nIn order to run ATL/MapReduce, please download the appropriate hadoop distribution [here](http://hadoop.apache.org/releases.html).\r\n\r\n###Hadoop cluster \r\nIt is also possible to run ATL/MapReduce on a hadoop cluster such as [CDH-Cloudera](http://www.cloudera.com/content/cloudera/en/products-and-services/cdh.html) or [Amazon Elastic MapReduce (EMR)](http://aws.amazon.com/fr/elasticmapreduce/).\r\n\r\nIn order to build the distribution files of the project, you must **re-create** the build.xml file by re-exporting it.\r\nThis is **necessary** to match your computer's configuration:\r\n\r\n1. Go to *File -> Export* and select *General / Ant Buildfiles*.\r\n2. Select  the project and press Finish.\r\n\r\nThe distribution JAR files and dependencies are automatically created and copied in the **dist** folder by executing the ``dist.emftvm`` and ``dist`` targets of the ``dist.xml`` ant script.\r\n\r\n**Please note that hints on the execution syntax are provided. For more information please check the run.bat/run.sh files in the dist folder**.\r\n\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}